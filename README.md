# Awesome-training-free-diffusion

## Contents

## Image synthesis

**Training-free diffusion model adaptation for variable-sized text-to-image synthesis** \
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/e0378e0c642b1d292fcb224e8d5a39b3-Paper-Conference.pdf)]

**Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model** \
[[Github](https://github.com/ecnuljzhang/brush-your-text)]
[[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28550/29069)]

**Enhancing Prompt Following with Visual Control Through Training-Free Mask-Guided Diffusion** \
[[Paper](https://arxiv.org/pdf/2404.14768)]

**Training-Free Consistent Text-to-Image Generation** \
[[Github](https://consistory-paper.github.io/)]
[[Paper](https://arxiv.org/abs/2402.03286)]

**Boundary Guided Learning-Free Semantic Control with Diffusion Models** \
[[Github](https://l-yezhu.github.io/BoundaryDiffusion/)]
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/f737da5ea0e122870fad209509f87d5b-Paper-Conference.pdf)]

**LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models** \
[[Github](https://github.com/Young98CN/LoRA_Composer)]
[[Paper](https://arxiv.org/pdf/2403.11627)]

**A Training-Free Plug-and-Play Watermark Framework For Stable Diffusion** \
[[Paper](https://arxiv.org/pdf/2404.05607)]

**BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion** \
[[Github](https://github.com/showlab/BoxDiff)]
[[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.pdf)]

## Diffusion Model Acceleration

**AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration** \
[[Github](https://github.com/lilijiangg/AutoDiffusion)]
[[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_AutoDiffusion_Training-Free_Optimization_of_Time_Steps_and_Architectures_for_Automated_ICCV_2023_paper.pdf)]
